
@online{british_psychological_society_code_2021,
	title = {Code of Ethics and Conduct},
	url = {https://www.bps.org.uk/news-and-policy/bps-code-ethics-and-conduct},
	author = {{British Psychological Society}},
	date = {2021},
	file = {Code of Ethics and Conduct | BPS:C\:\\Users\\Charlie\\Zotero\\storage\\R9WN3MQU\\bps-code-ethics-and-conduct.html:text/html},
}

@article{nichols_good-subject_2008,
	title = {The Good-Subject Effect: Investigating Participant Demand Characteristics},
	volume = {135},
	issn = {0022-1309},
	url = {https://doi.org/10.3200/GENP.135.2.151-166},
	doi = {10.3200/GENP.135.2.151-166},
	shorttitle = {The Good-Subject Effect},
	abstract = {Although researchers are often concerned with the presence of participant demand, few have directly examined effects of demand on participant behavior. Before beginning the present study, a confederate informed participants (N = 100) of the study's purported hypothesis. Participants then performed a laboratory task designed to evaluate the extent to which they would respond in ways that may confirm or disconfirm the hypothesis of the study. The authors found that participants tended to respond in ways that confirmed the hypothesis, yet this tendency depended on attitudes toward the experiment or experimenter and other individual differences. In addition, results suggested that suspicion probes may be ineffective in measuring participants' previous knowledge and suspicion. Findings indicate the need for more research and consideration of demand in the design of studies and analysis of data.},
	pages = {151--166},
	number = {2},
	journaltitle = {The Journal of General Psychology},
	author = {Nichols, Austin Lee and Maner, Jon K.},
	urldate = {2022-03-10},
	date = {2008-04-01},
	pmid = {18507315},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.3200/{GENP}.135.2.151-166},
	keywords = {demand characteristics, good-subject effect, previous knowledge},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\YPWP8MQ3\\Nichols and Maner - 2008 - The Good-Subject Effect Investigating Participant.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\4RW46NUA\\GENP.135.2.html:text/html},
}

@article{kang_prevention_2013,
	title = {The prevention and handling of the missing data},
	volume = {64},
	issn = {2005-6419},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/},
	doi = {10.4097/kjae.2013.64.5.402},
	abstract = {Even in a well-designed and controlled study, missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions. This manuscript reviews the problems and types of missing data, along with the techniques for handling missing data. The mechanisms by which missing data occurs are illustrated, and the methods for handling the missing data are discussed. The paper concludes with recommendations for the handling of missing data.},
	pages = {402--406},
	number = {5},
	journaltitle = {Korean Journal of Anesthesiology},
	shortjournal = {Korean J Anesthesiol},
	author = {Kang, Hyun},
	urldate = {2022-03-10},
	date = {2013-05},
	pmid = {23741561},
	pmcid = {PMC3668100},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\NTVNFVMX\\Kang - 2013 - The prevention and handling of the missing data.pdf:application/pdf},
}

@article{myers_goodbye_2011,
	title = {Goodbye, Listwise Deletion: Presenting Hot Deck Imputation as an Easy and Effective Tool for Handling Missing Data},
	volume = {5},
	issn = {1931-2458},
	url = {https://doi.org/10.1080/19312458.2011.624490},
	doi = {10.1080/19312458.2011.624490},
	shorttitle = {Goodbye, Listwise Deletion},
	abstract = {Missing data are a ubiquitous problem in quantitative communication research, yet the missing data handling practices found in most published work in communication leave much room for improvement. In this article, problems with current practices are discussed and suggestions for improvement are offered. Finally, hot deck imputation is suggested as a practical solution to many missing data problems. A computational tool for {SPSS} (Statistical Package for the Social Sciences) is presented that will enable communication researchers to easily implement hot deck imputation in their own analyses.},
	pages = {297--310},
	number = {4},
	journaltitle = {Communication Methods and Measures},
	author = {Myers, Teresa   A.},
	urldate = {2022-03-10},
	date = {2011-10-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/19312458.2011.624490},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\8DNG6J5S\\Myers - 2011 - Goodbye, Listwise Deletion Presenting Hot Deck Im.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\SWLSVJV5\\19312458.2011.html:text/html},
}

@article{crocker_contingencies_2003,
	title = {Contingencies of Self-Worth in College Students: Theory and Measurement},
	volume = {85},
	issn = {1939-1315},
	doi = {10.1037/0022-3514.85.5.894},
	shorttitle = {Contingencies of Self-Worth in College Students},
	abstract = {The Contingencies of Self-Worth Scale assesses 7 sources of self-esteem in college students: academics, appearance, approval from others, competition, family support, God's love, and virtue. In confirmatory factor analyses on data from 1,418 college students, a 7-factor model fit to the data acceptably well and significantly better than several plausible alternative models. The subscales all have high internal consistency, test-retest reliability, are distinct from other personality measures, and have a simplex structure arrayed on a continuum from external to internal sources of self-esteem. Contingencies of self-worth assessed prior to college predicted how students spent their time during their 1st year of college. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {894--908},
	number = {5},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Crocker, Jennifer and Luhtanen, Riia K. and Cooper, M. Lynne and Bouvrette, Alexandra},
	date = {2003},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Theories, Rating Scales, Self-Worth, Academic Self Concept, College Students, Competition, Factor Analysis, Factor Structure, Family, Morality, Peer Relations, Physical Appearance, Religious Beliefs, Social Support, Test Reliability},
}

@article{midgley_when_2021,
	title = {When every day is a high school reunion: Social media comparisons and self-esteem},
	volume = {121},
	issn = {1939-1315},
	doi = {10.1037/pspi0000336},
	shorttitle = {When every day is a high school reunion},
	abstract = {Although past research has shown that social comparisons made through social media contribute to negative outcomes, little is known about the nature of these comparisons (domains, direction, and extremity), variables that determine comparison outcomes (post valence, perceiver’s self-esteem), and how these comparisons differ from those made in other contexts (e.g., text messages, face-to-face interactions). In 4 studies (N = 798), we provide the first comprehensive analysis of how individuals make and respond to social comparisons on social media, using comparisons made in real-time while browsing news feeds (Study 1), experimenter-generated comparisons (Study 2), and comparisons made on social media versus in other contexts (Studies 3 and 4). More frequent and more extreme upward comparisons resulted in immediate declines in self-evaluations as well as cumulative negative effects on individuals’ state self-esteem, mood, and life satisfaction after a social media browsing session. Moreover, downward and lateral comparisons occurred less frequently and did little to mitigate upward comparisons’ negative effects. Furthermore, low self-esteem individuals were particularly vulnerable to making more frequent and more extreme upward comparisons on social media, which in turn threatened their already-lower self-evaluations. Finally, social media comparisons resulted in greater declines in self-evaluations than those made in other contexts. Together, these studies provide the first insights into the cumulative impact of multiple comparisons, clarify the role of self-esteem in online comparison processes, and demonstrate how the characteristics and impact of comparisons on social media differ from those made in other contexts. ({PsycInfo} Database Record (c) 2021 {APA}, all rights reserved)},
	pages = {285--307},
	number = {2},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Midgley, Claire and Thai, Sabrina and Lockwood, Penelope and Kovacheff, Chloe and Page-Gould, Elizabeth},
	date = {2021},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Self-Esteem, Life Satisfaction, News Media, Online Social Networks, Self-Evaluation, Social Comparison, Social Media, Text Messaging},
	file = {Submitted Version:C\:\\Users\\Charlie\\Zotero\\storage\\DLYMLUUQ\\Midgley et al. - 2021 - When every day is a high school reunion Social me.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\722CWCD6\\2020-59517-001.html:text/html},
}

@article{aust_seriousness_2013,
	title = {Seriousness checks are useful to improve data validity in online research},
	volume = {45},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-012-0265-2},
	doi = {10.3758/s13428-012-0265-2},
	abstract = {Nonserious answering behavior increases noise and reduces experimental power; it is therefore one of the most important threats to the validity of online research. A simple way to address the problem is to ask respondents about the seriousness of their participation and to exclude self-declared nonserious participants from analysis. To validate this approach, a survey was conducted in the week prior to the German 2009 federal election to the Bundestag. Serious participants answered a number of attitudinal and behavioral questions in a more consistent and predictively valid manner than did nonserious participants. We therefore recommend routinely employing seriousness checks in online surveys to improve data validity.},
	pages = {527--535},
	number = {2},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Aust, Frederik and Diedenhofen, Birk and Ullrich, Sebastian and Musch, Jochen},
	urldate = {2022-03-11},
	date = {2013-06-01},
	langid = {english},
	file = {Springer Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\ITZXB36U\\Aust et al. - 2013 - Seriousness checks are useful to improve data vali.pdf:application/pdf},
}

@article{szczygiel_not_2021,
	title = {Not Only Reliability! The Importance of the Ecological Validity of the Math Anxiety Questionnaire for Adults},
	volume = {38},
	issn = {1015-5759},
	url = {https://econtent.hogrefe.com/doi/full/10.1027/1015-5759/a000646},
	doi = {10.1027/1015-5759/a000646},
	abstract = {. The measurement of math anxiety in adults is justified based on observations that math anxiety in parents and teachers predicts children’s math anxiety and achievement. Although there are many very good math anxiety measures intended for children and adolescents, their usefulness (e.g., {AMAS}, {MARS}) for adults is debatable. The most important objection against using these scales for adults is their ecological validity. The measurement of anxiety associated with math tests, classes, teachers, and homework is adequate for students of science, technology, engineering, and mathematics ({STEM}), but not for students of social sciences and humanities ({HS}) and non-students (e.g., parents and preschool and early education teachers). In response to this gap, the Math Anxiety Questionnaire for Adults ({MAQA}) was developed; it is designed to measure math anxiety related to math problem-solving in various groups of adults (especially non-students and {HS} students, as well as {STEM} students). The content, construct, criterion, and ecological validity of the {MAQA} were tested, and its internal and test-retest reliability was established. The results confirm that the {MAQA} is a valid and reliable measurement of math anxiety; therefore, it may be recommended for use in various groups of adults (e.g., students, teachers, and parents).},
	pages = {78--90},
	number = {2},
	journaltitle = {European Journal of Psychological Assessment},
	author = {Szczygieł, Monika},
	urldate = {2022-03-11},
	date = {2021},
	note = {Publisher: Hogrefe Publishing},
	keywords = {adults, math anxiety, measurement, reliability, validity},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\WAPSMFAZ\\Szczygieł - 2022 - Not Only Reliability!.pdf:application/pdf},
}

@online{statista_social_2022,
	title = {Social media - Statistics \& Facts},
	url = {https://www.statista.com/topics/1164/social-networks/},
	shorttitle = {Topic},
	titleaddon = {Statista},
	author = {Statista},
	urldate = {2022-03-11},
	date = {2022},
	langid = {english},
}

@article{gosling_very_2003,
	title = {A very brief measure of the Big-Five personality domains},
	volume = {37},
	issn = {0092-6566},
	url = {https://www.sciencedirect.com/science/article/pii/S0092656603000461},
	doi = {10.1016/S0092-6566(03)00046-1},
	abstract = {When time is limited, researchers may be faced with the choice of using an extremely brief measure of the Big-Five personality dimensions or using no measure at all. To meet the need for a very brief measure, 5 and 10-item inventories were developed and evaluated. Although somewhat inferior to standard multi-item instruments, the instruments reached adequate levels in terms of: (a) convergence with widely used Big-Five measures in self, observer, and peer reports, (b) test–retest reliability, (c) patterns of predicted external correlates, and (d) convergence between self and observer ratings. On the basis of these tests, a 10-item measure of the Big-Five dimensions is offered for situations where very short measures are needed, personality is not the primary topic of interest, or researchers can tolerate the somewhat diminished psychometric properties associated with very brief measures.},
	pages = {504--528},
	number = {6},
	journaltitle = {Journal of Research in Personality},
	shortjournal = {Journal of Research in Personality},
	author = {Gosling, Samuel D and Rentfrow, Peter J and Swann, William B},
	urldate = {2022-03-11},
	date = {2003-12-01},
	langid = {english},
}

@article{robins_measuring_2001,
	title = {Measuring Global Self-Esteem: Construct Validation of a Single-Item Measure and the Rosenberg Self-Esteem Scale},
	volume = {27},
	issn = {0146-1672},
	url = {https://doi.org/10.1177/0146167201272002},
	doi = {10.1177/0146167201272002},
	shorttitle = {Measuring Global Self-Esteem},
	abstract = {Four studies examined the construct validity of two global self-esteem measures. In Studies 1 through 3, the Single-Item Self-Esteem Scale ({SISE}) and the Rosenberg Self-Esteem Scale ({RSE}) showed strong convergent validity for men and women, for different ethnic groups, and for both college students and community members. The {SISE} and the {RSE} had nearly identical correlations with a wide range of criterion measures, including domain-specific self-evaluations, self-evaluative biases, social desirability, personality, psychological and physical health, peer ratings of group behavior, academic outcomes, and demographic variables. Study 4 showed that the {SISE} had only moderate convergent validity in a sample of children. Overall, the findings support the reliability and validity of the {SISE} and suggest it can provide a practical alternative to the {RSE} in adult samples. More generally, the findings contribute to the research literature by further elaborating the nomological network of global self-esteem.},
	pages = {151--161},
	number = {2},
	journaltitle = {Personality and Social Psychology Bulletin},
	shortjournal = {Pers Soc Psychol Bull},
	author = {Robins, Richard W. and Hendin, Holly M. and Trzesniewski, Kali H.},
	urldate = {2022-03-11},
	date = {2001-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{brown_attitudinal_1990,
	title = {Attitudinal Body-Image Assessment: Factor Analysis of the Body-Self Relations Questionnaire},
	volume = {55},
	issn = {0022-3891},
	url = {https://doi.org/10.1080/00223891.1990.9674053},
	doi = {10.1080/00223891.1990.9674053},
	shorttitle = {Attitudinal Body-Image Assessment},
	abstract = {This article presents an analysis of the factor structure of the Body-Self Relations Questionnaire ({BSRQ}), an attitudinal body-image instrument. Random stratified samples, drawn from a national survey, included 1,064 females and 988 males. In order to evaluate the replicability of the {BSRQ} factor structure, separate split-sample factor analyses (principal components with varimax rotation) were conducted for each sex. Largely consistent with the conceptual basis of the {BSRQ}, the resultant factors derived from each analysis were: Appearance Evaluation, Appearance Orientation, Fitness Evaluation, Fitness Orientation, Health Evaluation, Health Orientation, and Illness Orientation. Subsequent concordance analyses revealed marked stability of the factor structure both within and between sexes. Females demonstrated somewhat greater differentiation of body-image attitudes than did males. The utility of the {BSRQ} is discussed relative to extant body-image measures.},
	pages = {135--144},
	number = {1},
	journaltitle = {Journal of Personality Assessment},
	author = {Brown, Timothy A. and Cash, Thomas F. and Mikulka, Peter J.},
	urldate = {2022-03-11},
	date = {1990-09-01},
	pmid = {2231236},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00223891.1990.9674053},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\SJHRYWQU\\Brown et al. - 1990 - Attitudinal Body-Image Assessment Factor Analysis.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\CNLL7B4N\\00223891.1990.html:text/html},
}

@article{parent_handling_2013,
	title = {Handling Item-Level Missing Data: Simpler Is Just as Good},
	volume = {41},
	issn = {0011-0000},
	url = {https://doi.org/10.1177/0011000012445176},
	doi = {10.1177/0011000012445176},
	shorttitle = {Handling Item-Level Missing Data},
	abstract = {The topic of missing data has been receiving increasing attention, with calls to apply advanced methods of handling missingness to counseling psychology research. The present study sought to assess whether advanced methods of handling item-level missing data performed equivalently to simpler methods in designs similar to those counseling psychologists typically engage in. Results of an initial preliminary analysis, an analysis using real-world data, and a series of simulation studies were used in the present investigation. Results indicated that available case analysis, mean substitution, and multiple imputation had similar results across low levels of missing data, though in data with higher levels of missing data and other problems (e.g., small sample size or scales with weak internal reliability) mean substitution produced inflation of correlation coefficients among items. The present results support the use of available case analysis when dealing with low-level item-level missingness.},
	pages = {568--600},
	number = {4},
	journaltitle = {The Counseling Psychologist},
	shortjournal = {The Counseling Psychologist},
	author = {Parent, Mike C.},
	urldate = {2022-03-12},
	date = {2013-05-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {methodology, quantitative, training},
	file = {SAGE PDF Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\LCIW9A27\\Parent - 2013 - Handling Item-Level Missing Data Simpler Is Just .pdf:application/pdf},
}

@book{rubin_multiple_2004,
	title = {Multiple Imputation for Nonresponse in Surveys},
	isbn = {978-0-471-65574-9},
	abstract = {Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. Also presents the background for Bayesian and frequentist theory. After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.},
	pagetotal = {326},
	publisher = {John Wiley \& Sons},
	author = {Rubin, Donald B.},
	date = {2004-06-09},
	langid = {english},
	note = {Google-Books-{ID}: {bQBtw}6rx\_mUC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@online{qualtrics_provo_ut_notitle_2022,
	url = {https://www.qualtrics.com/uk/},
	abstract = {Qualtrics empowers companies to capture and act on customer, product, brand \& employee experience insights in one place.},
	titleaddon = {Qualtrics},
	author = {{Qualtrics, Provo, UT}},
	urldate = {2022-03-12},
	date = {2022-03},
	langid = {british},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\XL73PKUD\\uk.html:text/html},
}

@book{robinson_measures_1991,
	title = {Measures of Personality and Social Psychological Attitudes},
	volume = {1},
	isbn = {978-1-4832-1984-4},
	shorttitle = {Measures of Personality and Social Psychological Attitudes},
	abstract = {Measures of Personality and Social Psychological Attitudes: Volume 1 in Measures of Social Psychological Attitudes Series provides a comprehensive guide to the most promising and useful measures of important social science concepts. This book is divided into 12 chapters and begins with a description of the Measures of Personality and Social Psychological Attitudes Project's background and the major criteria for scale construction. The subsequent chapters review measures of "response set"; the scales dealing with the most general affective states, including life satisfaction and happiness; and the measured of self-esteem. These topics are followed by discussions of measures of social anxiety, which is conceived a major inhibitor of social interaction, as well as the negative states of depression and loneliness. Other chapters examine the separate dimensions of alienation, the predictive value of interpersonal trust and attitudes in studies of occupational choice and racial attitude change, and the attitude scales related to locus of control. The final chapters look into the measures related to authoritarianism, androgyny, and values.This book is of great value to social and political scientists, psychologists, nurses, social workers, non-academic professionals, and students.},
	pagetotal = {769},
	publisher = {Academic Press},
	author = {Robinson, John P. and Shaver, Phillip R. and Wrightsman, Lawrence S.},
	date = {1991},
	langid = {english},
	note = {Google-Books-{ID}: {uOtFBQAAQBAJ}},
	keywords = {Family \& Relationships / Life Stages / General, Psychology / Developmental / General, Psychology / Developmental / Lifespan Development, Psychology / General},
}

@book{hair_multivariate_2019,
	location = {Australia},
	edition = {Eighth edition / Joseph F. Hair, Jr., William C. Black, Barry J. Babin, Rolph E. Anderson.},
	title = {Multivariate data analysis.},
	isbn = {978-1-4737-5669-4},
	abstract = {Multivariate Data Analysis is an applications-oriented introduction to multivariate analysis for the non-statistician. The eighth edition incorporates pivotal advances in technology that will assist students in gaining a firm understanding of statistical and managerial principles so as to develop a "comfort zone" not only for the statistical, but also the practical issues involved.},
	publisher = {Cengage},
	author = {Hair, Joseph F.},
	editora = {Black, William C. and Babin, Barry J. and Anderson, Rolph E.},
	editoratype = {collaborator},
	date = {2019},
	keywords = {Multivariate analysis},
}

@article{schriesheim_controlling_1981,
	title = {Controlling Acquiescence Response Bias by Item Reversals: The Effect on Questionnaire Validity},
	volume = {41},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316448104100420},
	doi = {10.1177/001316448104100420},
	shorttitle = {Controlling Acquiescence Response Bias by Item Reversals},
	abstract = {The prevailing conventional wisdom is that it is advisable to mix positively and negatively worded items in psychological measures to counteract acquiescence response bias. However, there has been virtually no unambiguous empirical evidence to support this recommendation. Thus, an experiment was conducted to evaluate the ability of subjects to respond accurately to both positive and reversed (negative) items on a questionnaire. Items from the {LBDQ}—{XII} Initiating Structure and Consideration subscales were used to create a written description of a fictitious manager. One hundred-fifty subjects, all upper-division business undergraduates, were given the written managerial description and then asked to complete a questionnaire containing the twenty Initiating Structure and Consideration items. The managerial descriptions were in two forms (to portray high and low Initiating Structure), and the questionnaires contained items in three forms (all positively worded, all negatively worded, and mixed). The data were evaluated using a one-way analysis of variance and post hoc t-tests. Significant differences in response accuracy were found between the item wording conditions. It was concluded that it may not be advisable to employ reversed (negatively-worded) items to control acquiescence response bias, as such changes may actually impair response accuracy.},
	pages = {1101--1114},
	number = {4},
	journaltitle = {Educational and Psychological Measurement},
	shortjournal = {Educational and Psychological Measurement},
	author = {Schriesheim, Chester A. and Hill, Kenneth D.},
	urldate = {2022-03-13},
	date = {1981-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\49VI4X44\\Schriesheim and Hill - 1981 - Controlling Acquiescence Response Bias by Item Rev.pdf:application/pdf},
}

@article{condon_note_2006,
	title = {A note on some item characteristics related to acquiescent responding},
	volume = {40},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886905003557},
	doi = {10.1016/j.paid.2005.07.019},
	abstract = {We proposed that the correlation between an item score and the total acquiescence score in a balanced scale is an index for measuring the extent to which the response to this item is affected by acquiescence. In an empirical study, which used the perceived stress scale, the values of this psychometric index were related to two objective ‘superficial structure’ characteristics: item length and item complexity. The items that were most affected by acquiescence according to the psychometric index were longer and more complex than the least affected items, in accordance with theoretical expectations.},
	pages = {403--407},
	number = {3},
	journaltitle = {Personality and Individual Differences},
	shortjournal = {Personality and Individual Differences},
	author = {Condon, Lorena and Ferrando, Pere J. and Demestre, Josep},
	urldate = {2022-03-13},
	date = {2006-02-01},
	langid = {english},
	keywords = {Acquiescence, Balanced scales, Item characteristics, Item-total correlations},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\WJGK7QVJ\\Condon et al. - 2006 - A note on some item characteristics related to acq.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\4PTMMX7P\\S0191886905003557.html:text/html},
}

@article{winkler_controlling_1982,
	title = {Controlling for Acquiescence Response Set in scale development},
	volume = {67},
	issn = {1939-1854},
	doi = {10.1037/0021-9010.67.5.555},
	abstract = {Acquiescence response set ({ARS}), the tendency to agree with questionnaire statements regardless of content, is especially problematic in scale development when attitude structure is not well known, because it heightens the correlations among items that are worded similarly, even when they are not conceptually related. A partial correlation technique is described for measuring and controlling for {ARS} using the method of matched pairs. 1,351 persons earned an {ARS} score from the frequency with which they agreed with pairs of items logically opposite. Principal-components analysis was then performed on the 1st-order interitem partial correlation matrix, controlling for {ARS} score. Evidence is presented that this procedure reduces the average interitem correlation among like-worded items, increases the average interitem correlation among differently worded items measuring the same concept, and produces a principal components solution that is more interpretable. These conclusions emerge from comparisons with analyses of untransformed attitude scores and attitude scores excluding Ss who demonstrated the greatest acquiescence. (12 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {555--561},
	number = {5},
	journaltitle = {Journal of Applied Psychology},
	author = {Winkler, John D. and Kanouse, David E. and Ware, John E.},
	date = {1982},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Test Construction, Attitude Measures, Response Bias, Statistical Correlation},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\ZLUQZ3P7\\1983-02414-001.html:text/html},
}

@article{podsakoff_common_2003,
	title = {Common method biases in behavioral research: A critical review of the literature and recommended remedies},
	volume = {88},
	issn = {1939-1854},
	doi = {10.1037/0021-9010.88.5.879},
	shorttitle = {Common method biases in behavioral research},
	abstract = {Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {879--903},
	number = {5},
	journaltitle = {Journal of Applied Psychology},
	author = {Podsakoff, Philip M. and {MacKenzie}, Scott B. and Lee, Jeong-Yeon and Podsakoff, Nathan P.},
	date = {2003},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Response Bias, Behavioral Sciences, Cognitive Processes, Error of Measurement, Experimental Methods, Research Setting, Statistical Analysis},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\G8WV9KIR\\2003-08045-010.html:text/html},
}

@report{paulsen_internal_2017,
	title = {Internal Consistency},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://scholarworks.iu.edu/dspace/handle/2022/24498},
	abstract = {One way to estimate reliability, specifically the internal consistency, of {FSSE} results is by calculating Cronbach’s alphas and intercorrelations for the {FSSE} scales. Internal consistency is the extent to which a group of items measure the same construct, as evidenced by how well they vary together, or intercorrelate. A high degree of internal consistency enables the researcher to interpret the composite score as a measure of the construct (Henson, 2001). Assuming the {FSSE} scales effectively measure an underlying construct, we would expect to find high estimates of their internal consistency.},
	institution = {Faculty Survey of Student Engagement},
	type = {Technical Report},
	author = {Paulsen, Justin and {BrckaLorenz}, Allison},
	urldate = {2022-03-13},
	date = {2017},
	langid = {english},
	note = {Accepted: 2019-10-05T18:20:08Z},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\N85E8JUV\\Paulsen and BrckaLorenz - 2017 - Internal Consistency.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\SGLRDTUM\\24498.html:text/html},
}

@article{kaiser_little_1974,
	title = {Little Jiffy, Mark Iv},
	volume = {34},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316447403400115},
	doi = {10.1177/001316447403400115},
	pages = {111--117},
	number = {1},
	journaltitle = {Educational and Psychological Measurement},
	shortjournal = {Educational and Psychological Measurement},
	author = {Kaiser, Henry F. and Rice, John},
	urldate = {2022-03-14},
	date = {1974-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\LW4ZM76S\\Kaiser and Rice - 1974 - Little Jiffy, Mark Iv.pdf:application/pdf},
}

@article{horn_rationale_1965,
	title = {A rationale and test for the number of factors in factor analysis},
	volume = {30},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289447},
	doi = {10.1007/BF02289447},
	abstract = {It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares “capitalization” on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted.},
	pages = {179--185},
	number = {2},
	journaltitle = {Psychometrika},
	shortjournal = {Psychometrika},
	author = {Horn, John L.},
	urldate = {2022-03-14},
	date = {1965-06-01},
	langid = {english},
	file = {Springer Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\UQICGCI3\\Horn - 1965 - A rationale and test for the number of factors in .pdf:application/pdf},
}

@article{fabrigar_evaluating_1999,
	title = {Evaluating the use of exploratory factor analysis in psychological research},
	volume = {4},
	issn = {1939-1463},
	doi = {10.1037/1082-989X.4.3.272},
	abstract = {Despite the widespread use of exploratory factor analysis in psychological research, researchers often make questionable decisions when conducting these analyses. This article reviews the major design and analytical decisions that must be made when conducting a factor analysis and notes that each of these decisions has important consequences for the obtained results. Recommendations that have been made in the methodological literature are discussed. Analyses of 3 existing empirical data sets are used to illustrate how questionable decisions in conducting factor analyses can yield problematic results. The article presents a survey of 2 prominent journals that suggests that researchers routinely conduct analyses using such questionable methods. The implications of these practices for psychological research are discussed, and the reasons for current practices are reviewed. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {272--299},
	number = {3},
	journaltitle = {Psychological Methods},
	author = {Fabrigar, Leandre R. and Wegener, Duane T. and {MacCallum}, Robert C. and Strahan, Erin J.},
	date = {1999},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Psychology, Factor Analysis, Evaluation, Experimentation, Exploratory Factor Analysis, Scientific Communication},
	file = {Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\B2C8C2AQ\\Fabrigar et al. - 1999 - Evaluating the use of exploratory factor analysis .pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\FUYSE2FD\\1999-03908-004.html:text/html},
}

@article{cattell_scree_1966,
	title = {The Scree Test For The Number Of Factors},
	volume = {1},
	issn = {0027-3171, 1532-7906},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327906mbr0102_10},
	doi = {10.1207/s15327906mbr0102_10},
	pages = {245--276},
	number = {2},
	journaltitle = {Multivariate Behavioral Research},
	shortjournal = {Multivariate Behavioral Research},
	author = {Cattell, Raymond B.},
	urldate = {2022-03-14},
	date = {1966-04},
	langid = {english},
	file = {Cattell - 1966 - The Scree Test For The Number Of Factors.pdf:C\:\\Users\\Charlie\\Zotero\\storage\\UIJYNRRJ\\Cattell - 1966 - The Scree Test For The Number Of Factors.pdf:application/pdf},
}

@article{courtney_determining_2013,
	title = {Determining the Number of Factors to Retain in {EFA}: Using the {SPSS} R-Menu v2 0 to Make More Judicious Estimations},
	volume = {18},
	rights = {© 2013. Notwithstanding the {ProQuest} Terms and Conditions, you may use this content in accordance with the associated terms available at https://scholarworks.umass.edu/pare/policies.html},
	url = {https://www.proquest.com/docview/2366799655/abstract/B50A6AA27AFC4CC0PQ/1},
	shorttitle = {Determining the Number of Factors to Retain in {EFA}},
	abstract = {Exploratory factor analysis ({EFA}) is a common technique utilized in the development of assessment instruments. The key question when performing this procedure is how to best estimate the number of factors to retain. This is especially important as under- or over-extraction may lead to erroneous conclusions. Although recent advancements have been made to answer the number of factors question, popular statistical packages do not come standard with these modern techniques. This paper details how to program {IBM} {SPSS} Statistics software ({SPSS}) to conveniently perform five modern techniques designed to estimate the number of factors to retain. By utilizing the five empirically-supported techniques illustrated in this article, researchers will be able to more judiciously model data.},
	pages = {8},
	journaltitle = {Practical Assessment, Research \& Evaluation},
	author = {Courtney, Matthew and Gordon, Ray},
	urldate = {2022-03-14},
	date = {2013},
	note = {Num Pages: 8
Place: College Park, United States
Publisher: Practical Assessment, Research and Evaluation, Inc.},
	keywords = {Statistical Analysis, Research Methodology},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\T6L8H54W\\Courtney and Gordon - 2013 - Determining the Number of Factors to Retain in EFA.pdf:application/pdf},
}

@article{norris_evaluating_2010,
	title = {Evaluating the Use of Exploratory Factor Analysis in Developmental Disability Psychological Research},
	volume = {40},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-009-0816-2},
	doi = {10.1007/s10803-009-0816-2},
	abstract = {Exploratory factor analysis ({EFA}) is a widely used but poorly understood statistical procedure. This paper described {EFA} and its methodological variations. Then, key methodological variations were used to evaluate {EFA} usage over a 10-year period in five leading developmental disabilities journals. Sixty-six studies were located and evaluated on multiple procedural variations. Only 35\% (n = 23) of studies used {EFA}; principal components analysis was the model used most often (n = 40, 61\%). Orthogonal rotation was used most often (n = 39, 59\%). A large portion of studies ran analyses with a subject: item ratio larger than 5:1 (n = 49, 74\%). Most researchers employed multiple criteria for retaining factors (n = 45, 68\%). Overall, results indicated that published recommendations and guidelines for the use of {EFA} are largely ignored.},
	pages = {8--20},
	number = {1},
	journaltitle = {Journal of Autism and Developmental Disorders},
	shortjournal = {J Autism Dev Disord},
	author = {Norris, Megan and Lecavalier, Luc},
	urldate = {2022-03-14},
	date = {2010-01-01},
	langid = {english},
	file = {Springer Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\2JAGF6DN\\Norris and Lecavalier - 2010 - Evaluating the Use of Exploratory Factor Analysis .pdf:application/pdf},
}

@article{kaiser_application_1960,
	title = {The application of electronic computers to factor analysis},
	volume = {20},
	issn = {1552-3888},
	doi = {10.1177/001316446002000116},
	abstract = {Electronic computers facilitate greatly carrying out factor analysis. Computers will help in solving the communality problem and the question of the number of factors as well as the question of arbitrary factoring and the problem of rotation. "Cloacal short-cuts will not be necessary and the powerful methods of Guttman will be feasible." A library of programs essential for factor analysis is described, and the use of medium sized computers as the {IBM} 650 deprecated for factor analysis. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {141--151},
	journaltitle = {Educational and Psychological Measurement},
	author = {Kaiser, Henry F.},
	date = {1960},
	note = {Place: {US}
Publisher: Sage Publications},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\LBT4XU5Z\\1960-06772-001.html:text/html},
}

@article{zwick_comparison_1986,
	title = {Comparison of five rules for determining the number of components to retain},
	volume = {99},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.99.3.432},
	abstract = {Investigated the performance of 5 methods for determining the number of components to retain—J. L. Horn's (see record 1965-13273-001) parallel analysis, W. F. Velicer's (see record 1977-00166-001) minimum average partial ({MAP}), R. B. Cattell's (see {PA}, Vol 41:969) scree test, M. S. Bartlett's (1950) chi-square test, and H. F. Kaiser's (see record 1960-06772-001) eigenvalue greater than 1 rule—across 7 systematically varied conditions (sample size, number of variables, number of components, component saturation, equal or unequal numbers of variables for each component, and the presence or absence of unique and complex variables). Five sample correlation matrices were generated at each of 2 sample sizes from the 48 known population correlation matrices representing 6 levels of component pattern complexity. Results indicate that the performance of the parallel analysis and {MAP} methods was generally the best across all situations; the scree test was generally accurate but variable; and Bartlett's chi-square test was less accurate and more variable than the scree test. Kaiser's method tended to severely overestimate the number of components. (65 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {432--442},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Zwick, William R. and Velicer, Wayne F.},
	date = {1986},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Factor Analysis, Statistical Correlation},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\SZMB94GF\\1986-21041-001.html:text/html},
}

@article{comrey_minimum_1962,
	title = {The Minimum Residual Method of Factor Analysis},
	volume = {11},
	issn = {0033-2941},
	url = {https://doi.org/10.2466/pr0.1962.11.1.15},
	doi = {10.2466/pr0.1962.11.1.15},
	pages = {15--18},
	number = {1},
	journaltitle = {Psychological Reports},
	shortjournal = {Psychol Rep},
	author = {Comrey, Andrew L.},
	urldate = {2022-03-15},
	date = {1962-08-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\AVSSA3HF\\Comrey - 1962 - The Minimum Residual Method of Factor Analysis.pdf:application/pdf},
}

@article{osborne_what_2015,
	title = {What is Rotating in Exploratory Factor Analysis?},
	volume = {20},
	abstract = {Exploratory factor analysis ({EFA}) is one of the most commonly-reported quantitative methodology in the social sciences, yet much of the detail regarding what happens during an {EFA} remains unclear. The goal of this brief technical note is to explore what " rotation " is, what exactly is rotating, and why we use rotation when performing {EFAs}. Some commentary about the relative utility and desirability of different rotation methods concludes the narrative. Those of us who regularly use exploratory factor analysis ({EFA}), one of the most commonly-used statistical techniques reported in the social sciences literature (e.g., Fabrigar, Wegener, {MacCallum}, \& Strahan, 1999; Osborne, Costello, \& Kellow, 2008), know that rotation happens, that there are different types of rotation, and hopefully that the goal of all rotation methods is to clarify results. But what exactly is rotating during an {EFA}? The goal of this article is to answer that simple question.},
	pages = {1--8},
	journaltitle = {Assessment},
	shortjournal = {Assessment},
	author = {Osborne, Jason},
	date = {2015-02-01},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\R6ML666K\\Osborne - 2015 - What is Rotating in Exploratory Factor Analysis.pdf:application/pdf},
}

@article{peterson_meta-analysis_2000,
	title = {A Meta-Analysis of Variance Accounted for and Factor Loadings in Exploratory Factor Analysis},
	volume = {11},
	issn = {1573-059X},
	url = {https://doi.org/10.1023/A:1008191211004},
	doi = {10.1023/A:1008191211004},
	abstract = {A meta-analysis of two factor analysis outcome measures, the percentage of variance accounted for and the average (absolute) factor loading, in 803 substantive factor analyses was undertaken. The average percentage of variance accounted for was 56.6\%, and the average (absolute) factor loading was 0.32. Number of variables factor analyzed, nature of the sample from which data were collected, sample size, number of factors extracted, and (minimal) number of scale categories employed influenced the percentage of variance accounted for in a factor analysis. Number of factors extracted, analytical approach, and number of variables analyzed influenced the average factor loading obtained in a factor analysis. Factor analysis of synthetic (random) data possessing the general structure as the observed data in the meta-analysis accounted for 50.2\% of the variance in the data and produced an average factor loading of 0.21. The latter figures imply that many factor analyses have produced outcome measures of questionable meaningfulness.},
	pages = {261--275},
	number = {3},
	journaltitle = {Marketing Letters},
	shortjournal = {Marketing Letters},
	author = {Peterson, Robert A.},
	urldate = {2022-03-16},
	date = {2000-08-01},
	langid = {english},
	file = {Springer Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\K63JIIFJ\\Peterson - 2000 - A Meta-Analysis of Variance Accounted for and Fact.pdf:application/pdf},
}

@article{costello_best_2005,
	title = {Best Practices in Exploratory Factor Analysis: Four Recommendations for Getting the Most From Your Analysis},
	volume = {10},
	shorttitle = {Best Practices in Exploratory Factor Analysis},
	abstract = {Exploratory factor analysis ({EFA}) is a complex, multi-step process. The goal of this paper is to collect, in one article, information that will allow researchers and practitioners to understand the various choices available through popular software packages, and to make decisions about "best practices" in exploratory factor analysis. In particular, this paper provides practical information on making decisions regarding (a) extraction, (b) rotation},
	pages = {1--9},
	journaltitle = {Practical Assessment, Research \& Evaluation},
	shortjournal = {Practical Assessment, Research \& Evaluation},
	author = {Costello, {AB} and Osborne, Jason},
	date = {2005-01-01},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\6DQHBHEV\\Costello and Osborne - 2005 - Best Practices in Exploratory Factor Analysis Fou.pdf:application/pdf},
}

@article{watkins_exploratory_2018,
	title = {Exploratory Factor Analysis: A Guide to Best Practice},
	volume = {44},
	issn = {0095-7984},
	url = {https://doi.org/10.1177/0095798418771807},
	doi = {10.1177/0095798418771807},
	shorttitle = {Exploratory Factor Analysis},
	abstract = {Exploratory factor analysis ({EFA}) is a multivariate statistical method that has become a fundamental tool in the development and validation of psychological theories and measurements. However, researchers must make several thoughtful and evidence-based methodological decisions while conducting an {EFA}, and there are a number of options available at each decision point, some better than others. Reviews of the professional literature have consistently found that many applications of {EFA} are marked by an injudicious choice of methods and incomplete reports. This article provides a systematic, evidence-based guide to the conduct of {EFA} studies that can be followed by researchers with modest statistical training, supplemented with an example to illustrate its application.},
	pages = {219--246},
	number = {3},
	journaltitle = {Journal of Black Psychology},
	shortjournal = {Journal of Black Psychology},
	author = {Watkins, Marley W.},
	urldate = {2022-03-16},
	date = {2018-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {measurement, validity, {EFA}, exploratory factor analysis, multivariate},
	file = {SAGE PDF Full Text:C\:\\Users\\Charlie\\Zotero\\storage\\AG8CCT2L\\Watkins - 2018 - Exploratory Factor Analysis A Guide to Best Pract.pdf:application/pdf},
}

@online{samuels_advice_2017,
	title = {Advice on Exploratory Factor Analysis},
	url = {https://www.researchgate.net/publication/319165677_Advice_on_Exploratory_Factor_Analysis},
	abstract = {Exploratory Factor Analysis ({EFA}) is a process which can be carried out in {SPSS} to validate scales of items in a questionnaire. The purpose of an {EFA} is to describe a multidimensional data set using fewer variables. Once a questionnaire has been validated, another process called Confirmatory Factor Analysis can be used. This is supported by {AMOS}, a ‘sister’ package to {SPSS}.},
	type = {Monograph},
	author = {Samuels, P.},
	urldate = {2022-03-16},
	date = {2017-06-09},
	langid = {english},
	note = {Num Pages: 7
Place: 9/06/2017
Publisher: {ResearchGate}},
	file = {Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\8VYKSEDT\\Samuels - 2017 - Advice on Exploratory Factor Analysis.pdf:application/pdf;Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\VTNSGFNC\\6076.html:text/html},
}

@article{howard_review_2016,
	title = {A Review of Exploratory Factor Analysis Decisions and Overview of Current Practices: What We Are Doing and How Can We Improve?},
	volume = {32},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2015.1087664},
	doi = {10.1080/10447318.2015.1087664},
	shorttitle = {A Review of Exploratory Factor Analysis Decisions and Overview of Current Practices},
	abstract = {Authors within the fields of cyberpsychology and human−computer interaction have demonstrated a particular interest in measurement and scale creation, and exploratory factor analysis ({EFA}) is an extremely important statistical method for these areas of research. Unfortunately, {EFA} requires several statistical and methodological decisions to which the best choices are often unclear. The current article reviews five primary decisions and provides direct suggestions for best practices. These decisions are (a) the data inspection techniques, (b) the factor analytic method, (c) the factor retention method, (d) the factor rotation method, and (e) the factor loading cutoff. Then the article reviews authors’ choices for these five {EFA} decisions in every relevant article within seven cyberpsychology and/or human–computer interaction journals. The results demonstrate that authors do not employ the recommended best practices for most decisions. Particularly, most authors do not inspect their data for violations of assumptions, apply inappropriate factor analytic methods, utilize outdated factor retention methods, and omit the justification for their factor rotation methods. Further, many authors omit altogether their {EFA} decisions. To rectify these concerns, the current article provides a step-by-step guide and checklist that authors can reference to ensure the use of recommended best practices. Together, the current article identifies concerns with current research and provides direct solutions to these concerns.},
	pages = {51--62},
	number = {1},
	journaltitle = {International Journal of Human–Computer Interaction},
	author = {Howard, Matt C.},
	urldate = {2022-03-16},
	date = {2016-01-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2015.1087664},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\85V3WS62\\10447318.2015.html:text/html},
}

@article{hu_cutoff_1999,
	title = {Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives},
	volume = {6},
	issn = {1070-5511},
	url = {https://doi.org/10.1080/10705519909540118},
	doi = {10.1080/10705519909540118},
	shorttitle = {Cutoff criteria for fit indexes in covariance structure analysis},
	abstract = {This article examines the adequacy of the “rules of thumb” conventional cutoff criteria and several new alternatives for various fit indexes used to evaluate model fit in practice. Using a 2‐index presentation strategy, which includes using the maximum likelihood ({ML})‐based standardized root mean squared residual ({SRMR}) and supplementing it with either Tucker‐Lewis Index ({TLI}), Bollen's (1989) Fit Index ({BL}89), Relative Noncentrality Index ({RNI}), Comparative Fit Index ({CFI}), Gamma Hat, {McDonald}'s Centrality Index (Mc), or root mean squared error of approximation ({RMSEA}), various combinations of cutoff values from selected ranges of cutoff criteria for the {ML}‐based {SRMR} and a given supplemental fit index were used to calculate rejection rates for various types of true‐population and misspecified models; that is, models with misspecified factor covariance(s) and models with misspecified factor loading(s). The results suggest that, for the {ML} method, a cutoff value close to .95 for {TLI}, {BL}89, {CFI}, {RNI}, and Gamma Hat; a cutoff value close to .90 for Mc; a cutoff value close to .08 for {SRMR}; and a cutoff value close to .06 for {RMSEA} are needed before we can conclude that there is a relatively good fit between the hypothesized model and the observed data. Furthermore, the 2‐index presentation strategy is required to reject reasonable proportions of various types of true‐population and misspecified models. Finally, using the proposed cutoff criteria, the {ML}‐based {TLI}, Mc, and {RMSEA} tend to overreject true‐population models at small sample size and thus are less preferable when sample size is small.},
	pages = {1--55},
	number = {1},
	journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Hu, Li‐tze and Bentler, Peter M.},
	urldate = {2022-03-18},
	date = {1999-01-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705519909540118},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\GXMLDIDK\\10705519909540118.html:text/html;Full Text PDF:C\:\\Users\\Charlie\\Zotero\\storage\\K7IVYSJH\\Hu and Bentler - 1999 - Cutoff criteria for fit indexes in covariance stru.pdf:application/pdf},
}

@online{field_discovering_2022,
	title = {Discovering Statistics Using R and {RStudio}},
	url = {https://us.sagepub.com/en-us/nam/discovering-statistics-using-r-and-rstudio/book261351},
	titleaddon = {{SAGE} Publications Inc},
	author = {Field, Andy P.},
	urldate = {2022-03-18},
	date = {2022-03-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\T7TBHY2Z\\book261351.html:text/html},
}

@book{mcdonald_test_1999,
	location = {Mahwah, {NJ}, {US}},
	title = {Test theory:  A unified treatment},
	isbn = {978-0-8058-3075-0},
	series = {Test theory:  A unified treatment},
	shorttitle = {Test theory},
	abstract = {This book is an outcome of {McDonald}'s experience in teaching a laboratory course on test theory in a university department of psychology. The object of such a course on test theory is to introduce students to the main quantitative concepts, methods, and computational techniques needed for the development, evaluation, and application of tests in the behavioral/social sciences, including educational tests. The implicit unifying principle throughout this book is a general nonlinear common factor model, which includes item response models as special cases, and includes also the (linear) common factor model as an approximation. The account of the field in this text is developed from that unifying perspective, while giving appropriate coverage of the conventional topics. This book is intended primarily for students of psychology, including educational psychology, and any other fields of social science where tests are constructed and used. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pagetotal = {xi, 485},
	publisher = {Lawrence Erlbaum Associates Publishers},
	author = {{McDonald}, Roderick P.},
	date = {1999},
	note = {Pages: xi, 485},
	keywords = {Statistical Tests, Theories},
	file = {Snapshot:C\:\\Users\\Charlie\\Zotero\\storage\\FT8CH9BN\\1999-02770-000.html:text/html},
}